{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e320c7a4",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "992abab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e45166",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af073786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset=torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "dataloaders = {'train' : trainloader, 'val' : testloader }\n",
    "dataset_sizes = { 'train' : len(trainset) , 'val' : len(testset) }\n",
    "\n",
    "class_names = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(class_names)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #GPU 사용가능 여부확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ccf579",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5396b94",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d41aa258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet18 모델 불러오기\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# resnet50에서 마지막에 출력되는 출력값들\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# input : num_ftrs, output : 10개의 클래스로 최종 추출\n",
    "model.fc = nn.Linear(num_ftrs, 10).cuda()\n",
    "\n",
    "# model을 gpu 설정\n",
    "model_ft = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d72575be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "def model_training(model, device, train_dataloader, optimizer, train_acc, train_losses):\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = F.cross_entropy(y_pred, target)\n",
    "\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct += y_pred.argmax(dim=1).eq(target).sum()\n",
    "        processed += len(data)\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "        train_acc.append(100*correct/processed)\n",
    "\n",
    "def model_testing(model, device, test_dataloader, test_acc, test_losses, misclassified = []):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for index, (data, target) in enumerate(test_dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            test_loss += F.cross_entropy(output, target)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset)))\n",
    "\n",
    "    test_acc.append(100. * correct / len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2e25f",
   "metadata": {},
   "source": [
    "# Train & Test the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afda5262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.08851327747106552 Batch_id=781 Accuracy=87.57: 100%|██████████| 782/782 [03:07<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0026, Accuracy: 9437/10000 (94.37%)\n",
      "\n",
      "EPOCHS : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.023596376180648804 Batch_id=781 Accuracy=96.54: 100%|██████████| 782/782 [03:08<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 9556/10000 (95.56%)\n",
      "\n",
      "EPOCHS : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.14374658465385437 Batch_id=781 Accuracy=98.57: 100%|██████████| 782/782 [03:08<00:00,  4.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 9595/10000 (95.95%)\n",
      "\n",
      "EPOCHS : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.03797980397939682 Batch_id=781 Accuracy=99.40: 100%|██████████| 782/782 [03:08<00:00,  4.14it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 9584/10000 (95.84%)\n",
      "\n",
      "EPOCHS : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.009305593557655811 Batch_id=781 Accuracy=99.71: 100%|██████████| 782/782 [03:12<00:00,  4.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 9604/10000 (96.04%)\n",
      "\n",
      "EPOCHS : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.07185526937246323 Batch_id=781 Accuracy=99.82: 100%|██████████| 782/782 [03:15<00:00,  4.01it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 9610/10000 (96.10%)\n",
      "\n",
      "EPOCHS : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.026421058923006058 Batch_id=781 Accuracy=99.87: 100%|██████████| 782/782 [03:14<00:00,  4.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 9630/10000 (96.30%)\n",
      "\n",
      "EPOCHS : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.021423084661364555 Batch_id=781 Accuracy=99.90: 100%|██████████| 782/782 [03:17<00:00,  3.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-05.\n",
      "\n",
      "Test set: Average loss: 0.0024, Accuracy: 9594/10000 (95.94%)\n",
      "\n",
      "EPOCHS : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.0002972630027215928 Batch_id=781 Accuracy=99.95: 100%|██████████| 782/782 [03:19<00:00,  3.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 9609/10000 (96.09%)\n",
      "\n",
      "EPOCHS : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.04367726668715477 Batch_id=781 Accuracy=99.96: 100%|██████████| 782/782 [03:21<00:00,  3.87it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 9608/10000 (96.08%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.05, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
    "# scheduler = StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "train_acc = []\n",
    "train_losses = []\n",
    "test_acc = []\n",
    "test_losses = []\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(f'EPOCHS : {i}')\n",
    "    model_training(model, device, trainloader, optimizer, train_acc, train_losses)\n",
    "    scheduler.step(train_losses[-1])\n",
    "    model_testing(model, device, testloader, test_acc, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395549db",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ce07145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model, \"./model.pt\")\n",
    "\n",
    "# save model state_dict\n",
    "torch.save(model.state_dict(), \"./model_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc14ca",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5180cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = torch.load(\"./model.pt\")\n",
    "\n",
    "# load model state_dict\n",
    "model2 = MyModel()\n",
    "model2.load_state_dict(torch.load(\"./model_state_dict.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
